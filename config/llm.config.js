/**
 * LLM Configuration Module
 * Supports multiple LLM providers: OpenAI, Google Gemini
 */

require('dotenv').config();
const { OpenAI } = require('openai');
const { GoogleGenerativeAI } = require('@google/generative-ai');
const logger = require('../services/logger');
const Groq = require('groq-sdk');

// Determine which LLM provider to use
const LLM_PROVIDER = process.env.LLM_PROVIDER || 'gemini'; // 'openai', 'gemini', or 'groq'

class LLMConfig {
    constructor() {
        this.provider = LLM_PROVIDER;
        this.initializeClients();
    }

    initializeClients() {
        // Initialize OpenAI
        if (this.provider === 'openai' && process.env.OPENAI_API_KEY) {
            this.openai = new OpenAI({
                apiKey: process.env.OPENAI_API_KEY
            });
            this.model = process.env.OPENAI_MODEL || 'gpt-4-turbo-preview';
            this.embeddingModel = process.env.OPENAI_EMBEDDING_MODEL || 'text-embedding-3-small';
        }

        // Initialize Google Gemini
        if (this.provider === 'gemini' && process.env.GEMINI_API_KEY) {
            this.gemini = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
            this.model = process.env.GEMINI_MODEL || 'gemini-1.5-flash';
        }

        // Initialize Groq (Llama)
        if (this.provider === 'groq' && process.env.GROQ_API_KEY) {
            this.groq = new Groq({
                apiKey: process.env.GROQ_API_KEY
            });
            this.model = process.env.GROQ_MODEL || 'llama-3.1-70b-versatile';
        }
    }

    /**
     * Generate text completion
     */
    async generateCompletion(prompt, options = {}) {
        try {
            if (this.provider === 'openai') {
                return await this.generateOpenAICompletion(prompt, options);
            } else if (this.provider === 'gemini') {
                return await this.generateGeminiCompletion(prompt, options);
            } else if (this.provider === 'groq') {
                return await this.generateGroqCompletion(prompt, options);
            }
        } catch (error) {
            console.error('LLM completion error:', error);
            throw error;
        }
    }

    /**
     * OpenAI completion
     */
    async generateOpenAICompletion(prompt, options = {}) {
        const response = await this.openai.chat.completions.create({
            model: this.model,
            messages: [
                {
                    role: 'system',
                    content: options.systemPrompt || 'You are an expert agricultural AI assistant.'
                },
                {
                    role: 'user',
                    content: prompt
                }
            ],
            temperature: options.temperature || 0.7,
            max_tokens: options.maxTokens || 1000,
            response_format: options.jsonMode ? { type: 'json_object' } : undefined
        });

        return response.choices[0].message.content;
    }

    /**
     * Google Gemini completion
     */
    async generateGeminiCompletion(prompt, options = {}) {
        const modelsToTry = [
            this.model,
            'gemini-2.5-flash',
            'gemini-1.5-flash',
            'gemini-flash-latest',
            'gemini-pro'
        ].filter(Boolean);

        const systemPrompt = options.systemPrompt || 'You are an expert agricultural AI assistant.';
        const fullPrompt = `${systemPrompt}\n\n${prompt}`;
        const generationConfig = {
            temperature: options.temperature || 0.7,
            maxOutputTokens: options.maxTokens || 1000,
            responseMimeType: options.jsonMode ? 'application/json' : undefined
        };

        let lastError;
        for (const modelName of modelsToTry) {
            try {
                logger.info(`Attempting Gemini completion with model: ${modelName}`);
                const model = this.gemini.getGenerativeModel({ model: modelName });

                const result = await model.generateContent({
                    contents: [{ role: 'user', parts: [{ text: fullPrompt }] }],
                    generationConfig
                });

                if (result && result.response) {
                    const text = result.response.text();
                    logger.info(`Successfully generated content with model: ${modelName}`);
                    return text;
                }
            } catch (err) {
                logger.warn(`Gemini model ${modelName} failed: ${err.message}`);
                lastError = err;

                // If it's a quota error (429), we could try another model,
                // but usually the quota is shared. Still, no harm in trying.
                continue;
            }
        }

        // DEMO MODE FALLBACK
        // If all models fail (likely due to quota), return a realistic mock response for the demo.
        logger.error(`All Gemini models failed. Using DEMO FALLBACK response.`);

        // Detect domain context from prompt to give relevant mock answer
        if (prompt.toLowerCase().includes('onion') || prompt.toLowerCase().includes('farmer')) {
            return `Based on current market trends and weather patterns in Karnataka:

1. **Harvest Timing**: With prices currently at â‚¹25/kg and rising, delaying harvest by 5-7 days could improve bulb maturity and potential returns.
2. **Weather**: Light rain is forecast next week. Ensure proper drainage in the field to prevent bulb rot.
3. **Storage**: Consider storing 40% of the yield in cold storage to sell during the peak demand in May.

*Note: This is a generated insight for demonstration purposes.*`;
        }

        if (prompt.toLowerCase().includes('storage') || prompt.toLowerCase().includes('capacity')) {
            return `Current storage analysis indicates:

1. **Utilization**: Cold storage is at 33% capacity. There is ample space for the upcoming harvest.
2. **Recommendation**: Prioritize loading "N-53" variety onions first as they have a shorter shelf life.
3. **Efficiency**: Energy usage spikes observed between 2 PM - 5 PM. Consider shifting cooling cycles to off-peak hours.

*Note: This is a generated insight for demonstration purposes.*`;
        }

        if (prompt.toLowerCase().includes('logistics') || prompt.toLowerCase().includes('truck')) {
            return `Logistics optimization report:

1. **Route Efficiency**: The Kalaburagi-Bangalore route is showing a 15% delay due to roadworks near Chitradurga.
2. **Fleet Status**: 2 vehicles are currently halted. Immediate maintenance check recommended.
3. **Cost Saving**: Consolidating smaller loads from nearby farms could reduce transport costs by 12%.

*Note: This is a generated insight for demonstration purposes.*`;
        }

        return `I apologize, but I'm currently experiencing high traffic. However, based on general agricultural best practices:

1. Ensure soil moisture levels are monitored daily.
2. checks for pest infestations early morning.
3. Keep updated with local mandi prices before scheduling transport.

*Note: System is in demo mode due to connection limits.*`;
    }

    /**
     * Groq (Llama) completion
     */
    async generateGroqCompletion(prompt, options = {}) {
        try {
            const systemPrompt = options.systemPrompt || 'You are an expert assistant specializing in Agriculture, Storage, and Logistics. Provide helpful, accurate information about farming practices, crop management, storage optimization, inventory management, logistics planning, and supply chain efficiency. Do not mention what AI model or technology you are powered by. Focus on answering the user\'s questions directly and professionally.';

            const completion = await this.groq.chat.completions.create({
                messages: [
                    { role: 'system', content: systemPrompt },
                    { role: 'user', content: prompt }
                ],
                model: this.model,
                temperature: options.temperature || 0.7,
                max_tokens: options.maxTokens || 1000,
            });

            return completion.choices[0]?.message?.content || '';
        } catch (error) {
            logger.error(`Groq completion error: ${error.message}`);
            throw error;
        }
    }

    /**
     * Generate embeddings for RAG
     */
    async generateEmbedding(text) {
        try {
            if (this.provider === 'openai') {
                const response = await this.openai.embeddings.create({
                    model: this.embeddingModel,
                    input: text
                });
                return response.data[0].embedding;
            } else if (this.provider === 'gemini') {
                // Use correct embedding model for Gemini
                const model = this.gemini.getGenerativeModel({ model: 'text-embedding-004' });
                const result = await model.embedContent(text);
                return result.embedding.values;
            }
        } catch (error) {
            console.error('Embedding generation error:', error.message);
            throw error;
        }
    }

    /**
     * Generate structured JSON output
     */
    async generateStructuredOutput(prompt, schema, options = {}) {
        const jsonPrompt = `${prompt}\n\nRespond ONLY with valid JSON matching this schema:\n${JSON.stringify(schema, null, 2)}`;

        const response = await this.generateCompletion(jsonPrompt, {
            ...options,
            jsonMode: true
        });

        try {
            return JSON.parse(response);
        } catch (error) {
            console.error('Failed to parse JSON response:', response);
            throw new Error('Invalid JSON response from LLM');
        }
    }
}

module.exports = new LLMConfig();
